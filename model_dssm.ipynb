{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import softmax\n",
    "from keras.layers import InputSpec, Layer, Input, Dense, merge, Conv1D\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "import gensim\n",
    "import gc\n",
    "import os\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =pd.read_csv('/home/kesci/input/bytedance/train_final.csv',names=['qId','q','aId','a','target'],nrows=62500000)\n",
    "test =pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv',names=['qId','q','aId','a'])\n",
    "test['a']=test['a'].apply(lambda x:x[:-1])\n",
    "target=train['target']\n",
    "embed_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(lower=False,char_level=False,split=' ')\n",
    "tokenizer.fit_on_texts(train['q'].tolist()+train['a'].tolist()+test['q'].tolist()+test['a'].tolist())\n",
    "train_q=tokenizer.texts_to_sequences(train['q'])\n",
    "train_a=tokenizer.texts_to_sequences(train['a'])\n",
    "test_q=tokenizer.texts_to_sequences(test['q'])\n",
    "test_a=tokenizer.texts_to_sequences(test['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train,test\n",
    "gc.collect()\n",
    "maxlen=40\n",
    "train_q=pad_sequences(train_q,maxlen=40,value=0)\n",
    "train_a=pad_sequences(train_a,maxlen=40,value=0)\n",
    "test_q=pad_sequences(test_q,maxlen=40,value=0)\n",
    "test_a=pad_sequences(test_a,maxlen=40,value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=750000\n",
    "embedding_matrix=np.zeros((max_features,embed_size))\n",
    "from gensim.models import Word2Vec\n",
    "w2vmodel=Word2Vec.load('/home/kesci/work/w2vfinal_all.model')\n",
    "num_oov=0\n",
    "for word in tokenizer.word_index:\n",
    "    try:\n",
    "        embedding_matrix[tokenizer.word_index[word]]=w2vmodel[word]\n",
    "    except:\n",
    "        num_oov+=1\n",
    "print('oov num is '+str(num_oov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_fea=pd.read_csv('/home/kesci/work/train_feature_freq_final.csv',nrows=62500000)\n",
    "test_fea=pd.read_csv('/home/kesci/work/test_feature_freq_final.csv')\n",
    "\n",
    "train_fea=train_fea.fillna(0)\n",
    "test_fea=test_fea.fillna(0)\n",
    "train_fea[np.isinf(train_fea)] =0\n",
    "test_fea[np.isinf(test_fea)] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='%s_W'%self.name,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='%s_b'%self.name,\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DSSM(emb_matrix, max_sequence_length,feature_shape, lstmsize=90):\n",
    "    embedding = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=max_sequence_length,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    input1 = Input(shape=(max_sequence_length,))\n",
    "    input2 = Input(shape=(max_sequence_length,))\n",
    "    \n",
    "    lstm0 = CuDNNLSTM(lstmsize,return_sequences = True)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(lstmsize))\n",
    "    lstm2 = CuDNNLSTM(lstmsize)\n",
    "    att1 = Attention(max_sequence_length)\n",
    "    den = Dense(64,activation = 'tanh')\n",
    "\n",
    "    # att1 = Lambda(lambda x: K.max(x,axis = 1))\n",
    "\n",
    "    v1 = embedding(input1)\n",
    "    v2 = embedding(input2)\n",
    "    v11 = lstm1(v1)\n",
    "    v22 = lstm1(v2)\n",
    "    v1ls = lstm2(lstm0(v1))\n",
    "    v2ls = lstm2(lstm0(v2))\n",
    "    v1 = Concatenate(axis=1)([att1(v1),v11])\n",
    "    v2 = Concatenate(axis=1)([att1(v2),v22])\n",
    "    fea = Input(shape=(feature_shape,))\n",
    "    feax = Dense(128, activation='relu')(fea)\n",
    "    mul = Multiply()([v1,v2])\n",
    "    sub = Lambda(lambda x: K.abs(x))(Subtract()([v1,v2]))\n",
    "    maximum = Maximum()([Multiply()([v1,v1]),Multiply()([v2,v2])])\n",
    "    sub2 = Lambda(lambda x: K.abs(x))(Subtract()([v1ls,v2ls]))\n",
    "    \n",
    "    matchlist = Concatenate(axis=1)([mul,sub,maximum,sub2,feax])\n",
    "    matchlist = Dropout(0.05)(matchlist)\n",
    "\n",
    "    matchlist = Concatenate(axis=1)([Dense(32,activation = 'relu')(matchlist),Dense(128,activation = 'sigmoid')(matchlist)])\n",
    "    res = Dense(1, activation = 'sigmoid')(matchlist)\n",
    "\n",
    "    model = Model(inputs=[input1, input2,fea ], outputs=res)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim=train_fea.shape[1]\n",
    "num_class=1\n",
    "sub1 = np.zeros((test_a.shape[0],1))\n",
    "\n",
    "count = 0\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "for i, (trn_idx, val_idx) in enumerate(folds.split(train_q)):\n",
    "    print(\"FOLD_ \", count + 1)\n",
    "    filepath = \"/home/kesci/work/nnDA_best_model%d.h5\" % count\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.8, patience=1, min_lr=0.00001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=2, verbose=1, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "    model_nn =decomposable_attention(embedding_matrix, max_sequence_length=maxlen,feature_shape=feature_dim)\n",
    "    X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl,X_fea_tr,X_fea_vl = train_q[trn_idx], train_q[val_idx], train_a[\n",
    "        trn_idx], train_a[val_idx], target[trn_idx], target[val_idx],train_fea.iloc[trn_idx],train_fea.iloc[val_idx]\n",
    "    hist = model_nn.fit([X_tr, X_tr2,X_fea_tr], y_tr, batch_size=1024, epochs=10, validation_data=([X_vl, X_vl2,X_fea_vl], y_vl),\n",
    "                         callbacks=callbacks, verbose=1, shuffle=True)\n",
    "    print('load_wight')\n",
    "    model_nn.load_weights(filepath)\n",
    "    print('start to predict')\n",
    "    sub1 = model_nn.predict([test_q, test_a,test_fea], batch_size=2024)\n",
    "    break\n",
    "sub=pd.DataFrame(sub1.reshape(-1,1))\n",
    "sub.to_csv('/home/kesci/work/nndssm_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
