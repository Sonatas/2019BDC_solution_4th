{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import softmax\n",
    "from keras.layers import InputSpec, Layer, Input, Dense, merge, Conv1D\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "import gensim\n",
    "import gc\n",
    "import os\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, savedir, save_name=\"word2vector.model\"):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        self.save_path = os.path.join(savedir, save_name)\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "train =pd.read_csv('/home/kesci/input/bytedance/train_final.csv',names=['qId','q','aId','a','target'],nrows=62500000)\n",
    "test =pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv',names=['qId','q','aId','a'])\n",
    "test['a']=test['a'].apply(lambda x:x[:-1])\n",
    "target=train['target']\n",
    "embed_size=200\n",
    "tokenizer=Tokenizer(lower=False,char_level=False,split=' ')\n",
    "tokenizer.fit_on_texts(train['q'].tolist()+train['a'].tolist()+test['q'].tolist()+test['a'].tolist())\n",
    "train_q=tokenizer.texts_to_sequences(train['q'])\n",
    "train_a=tokenizer.texts_to_sequences(train['a'])\n",
    "test_q=tokenizer.texts_to_sequences(test['q'])\n",
    "test_a=tokenizer.texts_to_sequences(test['a'])\n",
    "del train,test\n",
    "gc.collect()\n",
    "maxlen=40\n",
    "train_q=pad_sequences(train_q,maxlen=40,value=0)\n",
    "train_a=pad_sequences(train_a,maxlen=40,value=0)\n",
    "test_q=pad_sequences(test_q,maxlen=40,value=0)\n",
    "test_a=pad_sequences(test_a,maxlen=40,value=0)\n",
    "max_features=750000\n",
    "embedding_matrix=np.zeros((max_features,embed_size))\n",
    "from gensim.models import Word2Vec\n",
    "w2vmodel=Word2Vec.load('/home/kesci/work/w2vfinal_all.model')\n",
    "num_oov=0\n",
    "for word in tokenizer.word_index:\n",
    "    try:\n",
    "        embedding_matrix[tokenizer.word_index[word]]=w2vmodel[word]\n",
    "    except:\n",
    "        num_oov+=1\n",
    "print('oov num is '+str(num_oov))\n",
    "\n",
    "\n",
    "train_fea=pd.read_csv('/home/kesci/work/train_feature_freq_final.csv',nrows=62500000)\n",
    "test_fea=pd.read_csv('/home/kesci/work/test_feature_freq_final.csv')\n",
    "\n",
    "train_fea=train_fea.fillna(0)\n",
    "test_fea=test_fea.fillna(0)\n",
    "train_fea[np.isinf(train_fea)] =0\n",
    "test_fea[np.isinf(test_fea)] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(emb_matrix, max_sequence_length,feature_shape):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=max_sequence_length,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(max_sequence_length,))\n",
    "    seq2 = Input(shape=(max_sequence_length,))\n",
    "    \n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "   # man_distance = ManDist()([mergea, mergeb])\n",
    "\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    \n",
    "    fea = Input(shape=(feature_shape,))\n",
    "    feax = Dense(128, activation='relu')(fea)\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "    # merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    merge = concatenate([diff, mul,feax])\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.3)(merge)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "    # model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model = Model(inputs=[seq1, seq2 ,fea], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class=1\n",
    "sub1 = np.zeros((test_a.shape[0],1))\n",
    "score = []\n",
    "count = 0\n",
    "feature_shape=train_fea.shape[1]\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "for i, (trn_idx, val_idx) in enumerate(folds.split(train_q)):\n",
    "    print(\"FOLD_ \", count + 1)\n",
    "    filepath = \"/home/kesci/work/nn1_best_model%d.h5\" % count\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.8, patience=1, min_lr=0.00001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=2, verbose=1, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "    model_nn =build_model(embedding_matrix, 40,feature_shape)\n",
    "    X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl,X_fea_tr,X_fea_vl = train_q[trn_idx], train_q[val_idx], train_a[\n",
    "        trn_idx], train_a[val_idx], target[trn_idx], target[val_idx],train_fea.iloc[trn_idx],train_fea.iloc[val_idx]\n",
    "    gc.collect()\n",
    "    hist = model_nn.fit([X_tr, X_tr2,X_fea_tr], y_tr, batch_size=512, epochs=15, validation_data=([X_vl, X_vl2,X_fea_vl], y_vl),\n",
    "                         callbacks=callbacks, verbose=1, shuffle=True)\n",
    "    print('load_wight')\n",
    "    model_nn.load_weights(filepath)\n",
    "    print('start to predict')\n",
    "    sub1 = model_nn.predict([test_q, test_a,test_fea], batch_size=2024)\n",
    "    count += 1\n",
    "    break\n",
    "sub1=pd.DataFrame(sub1)\n",
    "sub1.to_csv('/home/kesci/work/nn1_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
