{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "df_train=pd.read_csv('/home/kesci/input/bytedance/train_final.csv',names=['qId','q','aId','a','target'],nrows=50000000)\n",
    "df_test=pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv',names=['qId','q','aId','a'])\n",
    "df_test['a']=df_test['a'].apply(lambda x:x[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qs = pd.Series(df_train['q'].tolist() + df_train['a'].tolist())\n",
    "import gc\n",
    "gc.collect()\n",
    "words = (\" \".join(train_qs)).split()#这里可以改进\n",
    "counts = Counter(words)\n",
    "del words\n",
    "gc.collect()\n",
    "del train_qs\n",
    "gc.collect()\n",
    "train_qs=pd.Series(df_test['q'].tolist())\n",
    "words = (\" \".join(train_qs)).split()#这里可以改进\n",
    "counts.update(words)\n",
    "del train_qs,words\n",
    "gc.collect()\n",
    "train_qs=pd.Series(df_test['a'].tolist())\n",
    "words = (\" \".join(train_qs)).split()#这里可以改进\n",
    "counts.update(words)\n",
    "#内存限制，逐步更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_qs))\n",
    "del train_qs,words\n",
    "gc.collect()\n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "save_obj(counts,'dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "counts=load_obj('dict')\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "        return 0 if count < min_count else 1 / (count + eps)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "del words,train_qs\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_share_fun(df_):\n",
    "    print('start')\n",
    "    result=[]\n",
    "    \n",
    "    def word_shares(q,a):\n",
    "        q1_list = q.split()\n",
    "        q1 = set(q1_list)\n",
    "        q1words = q1\n",
    "        if len(q1words) == 0:\n",
    "            result.append([0,0,0,0,0,0])\n",
    "        q2_list = a.split()\n",
    "        q2 = set(q2_list)\n",
    "        q2words = q2\n",
    "        if len(q2words) == 0:\n",
    "            result.append([0,0,0,0,0,0])\n",
    "    \n",
    "        words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "    \n",
    "        q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "        q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "    \n",
    "        shared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "    \n",
    "        shared_words = q1words.intersection(q2words)\n",
    "        shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "        q1_weights = [weights.get(w, 0) for w in q1words]\n",
    "        q2_weights = [weights.get(w, 0) for w in q2words]\n",
    "        total_weights = q1_weights + q2_weights\n",
    "    \n",
    "        R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "        R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "\n",
    "        Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "        Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "        if len(q1_2gram) + len(q2_2gram) == 0:\n",
    "            R2gram = 0\n",
    "        else:\n",
    "            R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "        result.append([R1, R2, len(shared_words),R2gram, Rcosine, words_hamming])\n",
    "    df_.apply(lambda row: word_shares(row['q'], row['a']), axis=1)\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_train[['q','a']].iloc[0:10000000,]\n",
    "df2=df_train[['q','a']].iloc[10000000:20000000,]\n",
    "df3=df_train[['q','a']].iloc[20000000:30000000,]\n",
    "df4=df_train[['q','a']].iloc[30000000:40000000,]\n",
    "df5=df_train[['q','a']].iloc[40000000:50000000,]\n",
    "\n",
    "df6=df_test[['q','a']].iloc[0:10000000,]\n",
    "df7=df_test[['q','a']].iloc[10000000:20000000,]\n",
    "df8=df_test[['q','a']].iloc[20000000:30000000,]\n",
    "df9=df_test[['q','a']].iloc[30000000:40000000,]\n",
    "df10=df_test[['q','a']].iloc[40000000:50000000,]\n",
    "df11=df_test[['q','a']].iloc[50000000:60000000,]\n",
    "df12=df_test[['q','a']].iloc[60000000:70000000,]\n",
    "df13=df_test[['q','a']].iloc[70000000:80000000,]\n",
    "df14=df_test[['q','a']].iloc[80000000:90000000,]\n",
    "df15=df_test[['q','a']].iloc[90000000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train,df_test\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "print(cpu_count())\n",
    "p= Pool(15)\n",
    "df1=p.apply_async(word_share_fun, args=(df1,))\n",
    "df2=p.apply_async(word_share_fun, args=(df2,))\n",
    "df3=p.apply_async(word_share_fun, args=(df3,))\n",
    "df4=p.apply_async(word_share_fun, args=(df4,))\n",
    "df5=p.apply_async(word_share_fun, args=(df5,))\n",
    "df6=p.apply_async(word_share_fun, args=(df6,))\n",
    "df7=p.apply_async(word_share_fun, args=(df7,))\n",
    "df8=p.apply_async(word_share_fun, args=(df8,))\n",
    "df9=p.apply_async(word_share_fun, args=(df9,))\n",
    "df10=p.apply_async(word_share_fun, args=(df10,))\n",
    "df11=p.apply_async(word_share_fun, args=(df11,))\n",
    "df12=p.apply_async(word_share_fun, args=(df12,))\n",
    "df13=p.apply_async(word_share_fun, args=(df13,))\n",
    "df14=p.apply_async(word_share_fun, args=(df14,))\n",
    "df15=p.apply_async(word_share_fun, args=(df15,))\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.get()\n",
    "df2=df2.get()\n",
    "df3=df3.get()\n",
    "df4=df4.get()\n",
    "df5=df5.get()\n",
    "df6=df6.get()\n",
    "df7=df7.get()\n",
    "df8=df8.get()\n",
    "df9=df9.get()\n",
    "df10=df10.get()\n",
    "df11=df11.get()\n",
    "df12=df12.get()\n",
    "df13=df13.get()\n",
    "df14=df14.get()\n",
    "df15=df15.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([df1,df2,df3,df4,df5])\n",
    "test=pd.concat([df6,df7,df8,df9,df10,df11,df12,df13,df14,df15])\n",
    "train.to_csv('/home/kesci/work/train_feature_ws_final.csv',index=False)\n",
    "test.to_csv('/home/kesci/work/test_feature_ws_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
